// SPDX-License-Identifier: Apache-2.0
// Copyright 2023 The Prime Citizens
//
// Copyright 2014 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package stdmap

import (
	"unsafe"

	"github.com/primecitizens/pcz/std/core/abi"
	"github.com/primecitizens/pcz/std/core/arch"
	"github.com/primecitizens/pcz/std/core/assert"
	"github.com/primecitizens/pcz/std/core/thread"
)

func MakeSmall() *hmap {
	h := new(hmap)
	h.Hash0 = thread.G().G().Rand32()
	return h
}

func Make(t *abi.MapType, hint int, h *hmap) *hmap {
	assert.TODO()
	return nil
	// mem, overflow := math.MulUintptr(uintptr(hint), t.Bucket.Size_)
	//
	//	if overflow || mem > os.MaxAlloc {
	//		hint = 0
	//	}
	//
	// // initialize Hmap
	//
	//	if h == nil {
	//		h = new(hmap)
	//	}
	//
	// h.Hash0 = rand.Fastrand()
	//
	// // Find the size parameter B which will hold the requested # of elements.
	// // For hint < 0 overLoadFactor returns false since hint < bucketCnt.
	// B := uint8(0)
	//
	//	for overLoadFactor(hint, B) {
	//		B++
	//	}
	//
	// h.B = B
	//
	// // allocate initial hash table
	// // if B == 0, the buckets field is allocated lazily later (in mapassign)
	// // If hint is large zeroing this memory could take a while.
	//
	//	if h.B != 0 {
	//		var nextOverflow *bmap
	//		h.buckets, nextOverflow = makeBucketArray(t, h.B, nil)
	//		if nextOverflow != nil {
	//			h.Extra = new(mapextra)
	//			h.Extra.nextOverflow = nextOverflow
	//		}
	//	}
	//
	// return h
}

const (
	// Maximum number of key/elem pairs a bucket can hold.
	bucketCntBits = 3
	bucketCnt     = 1 << bucketCntBits

	// Maximum average load of a bucket that triggers growth is 6.5.
	// Represent as loadFactorNum/loadFactorDen, to allow integer math.
	loadFactorNum = 13
	loadFactorDen = 2

	// Maximum key or elem size to keep inline (instead of mallocing per element).
	// Must fit in a uint8.
	// Fast versions cannot handle big elems - the cutoff size for
	// fast versions in cmd/compile/internal/gc/walk.go must be at most this elem.
	maxKeySize  = 128
	maxElemSize = 128

	// data offset should be the size of the bmap struct, but needs to be
	// aligned correctly. For amd64p32 this means 64-bit alignment
	// even though pointers are 32 bit.
	dataOffset = unsafe.Offsetof(struct {
		b bmap
		v int64
	}{}.v)

	// Possible tophash values. We reserve a few possibilities for special marks.
	// Each bucket (including its overflow buckets, if any) will have either all or none of its
	// entries in the evacuated* states (except during the evacuate() method, which only happens
	// during map writes and thus no one else can observe the map during that time).
	emptyRest      = 0 // this cell is empty, and there are no more non-empty cells at higher indexes or overflows.
	emptyOne       = 1 // this cell is empty
	evacuatedX     = 2 // key/elem is valid.  Entry has been evacuated to first half of larger table.
	evacuatedY     = 3 // same as above, but evacuated to second half of larger table.
	evacuatedEmpty = 4 // cell is empty, bucket is evacuated.
	minTopHash     = 5 // minimum tophash for a normal filled cell.

	// flags
	iterator     = 1 // there may be an iterator using buckets
	oldIterator  = 2 // there may be an iterator using oldbuckets
	hashWriting  = 4 // a goroutine is writing to the map
	sameSizeGrow = 8 // the current map growth is to a new map of the same size

	// sentinel bucket ID for iterator checks
	noCheck = 1<<(8*arch.PtrSize) - 1
)

//
// // makeBucketArray initializes a backing array for map buckets.
// // 1<<b is the minimum number of buckets to allocate.
// // dirtyalloc should either be nil or a bucket array previously
// // allocated by makeBucketArray with the same t and b parameters.
// // If dirtyalloc is nil a new backing array will be alloced and
// // otherwise dirtyalloc will be cleared and reused as backing array.
// func makeBucketArray(t *abi.MapType, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap) {
// 	base := bucketShift(b)
// 	nbuckets := base
// 	// For small b, overflow buckets are unlikely.
// 	// Avoid the overhead of the calculation.
// 	if b >= 4 {
// 		// Add on the estimated number of overflow buckets
// 		// required to insert the median number of elements
// 		// used with this value of b.
// 		nbuckets += bucketShift(b - 4)
// 		sz := t.Bucket.Size_ * nbuckets
// 		up := alloc.RoundupSize(sz)
// 		if up != sz {
// 			nbuckets = up / t.Bucket.Size_
// 		}
// 	}
//
// 	if dirtyalloc == nil {
// 		buckets = newarray(t.Bucket, int(nbuckets))
// 	} else {
// 		// dirtyalloc was previously generated by
// 		// the above newarray(t.bucket, int(nbuckets))
// 		// but may not be empty.
// 		buckets = dirtyalloc
// 		size := t.Bucket.Size_ * nbuckets
// 		if t.Bucket.PtrBytes != 0 {
// 			mem.MemclrHasPointers(buckets, size)
// 		} else {
// 			mem.MemclrNoHeapPointers(buckets, size)
// 		}
// 	}
//
// 	if base != nbuckets {
// 		// We preallocated some overflow buckets.
// 		// To keep the overhead of tracking these overflow buckets to a minimum,
// 		// we use the convention that if a preallocated overflow bucket's overflow
// 		// pointer is nil, then there are more available by bumping the pointer.
// 		// We need a safe non-nil pointer for the last overflow bucket; just use buckets.
// 		nextOverflow = (*bmap)(unsafe.Add(buckets, base*uintptr(t.BucketSize)))
// 		last := (*bmap)(unsafe.Add(buckets, (nbuckets-1)*uintptr(t.BucketSize)))
// 		last.setoverflow(t, (*bmap)(buckets))
// 	}
// 	return buckets, nextOverflow
// }
//
// // bucketShift returns 1<<b, optimized for code generation.
// func bucketShift(b uint8) uintptr {
// 	// Masking the shift amount allows overflow checks to be elided.
// 	return uintptr(1) << (b & (arch.PtrSize*8 - 1))
// }
//
// // bucketMask returns 1<<b - 1, optimized for code generation.
// func bucketMask(b uint8) uintptr {
// 	return bucketShift(b) - 1
// }
//
// // overLoadFactor reports whether count items placed in 1<<B buckets is over loadFactor.
// func overLoadFactor(count int, B uint8) bool {
// 	return count > bucketCnt && uintptr(count) > loadFactorNum*(bucketShift(B)/loadFactorDen)
// }
//
// // newarray allocates an array of n elements of type typ.
// func newarray(typ *abi.Type, n int) unsafe.Pointer {
// 	if n == 1 {
// 		return alloc.MallocGC(typ.Size_, typ, true)
// 	}
// 	mem, overflow := math.MulUintptr(typ.Size_, uintptr(n))
// 	if overflow || mem > os.MaxAlloc || n < 0 {
// 		panic(cerr.String("runtime: allocation size out of range"))
// 	}
//
// 	return alloc.MallocGC(mem, typ, true)
// }
